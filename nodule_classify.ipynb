{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nodule_classify.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTNRoNuu1VKS"
      },
      "outputs": [],
      "source": [
        "'''Train CIFAR10 with PyTorch.'''\n",
        "from __future__ import print_function\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "from models import *\n",
        "from utils import progress_bar\n",
        "from torch.autograd import Variable\n",
        "import logging\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CROPSIZE = 17\n",
        "gbtdepth = 1\n",
        "fold = 5\n",
        "blklst = []#['1.3.6.1.4.1.14519.5.2.1.6279.6001.121993590721161347818774929286-388', \\\n",
        "    # '1.3.6.1.4.1.14519.5.2.1.6279.6001.121993590721161347818774929286-389', \\\n",
        "    # '1.3.6.1.4.1.14519.5.2.1.6279.6001.132817748896065918417924920957-660']\n",
        "logging.basicConfig(filename='log-'+str(fold), level=logging.INFO)\n",
        "parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
        "parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
        "parser.add_argument('--resume', '-r', action='store_true', help='resume from checkpoint')\n",
        "args = parser.parse_args()\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "best_acc = 0  # best test accuracy\n",
        "best_acc_gbt = 0\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "# Cal mean std\n",
        "preprocesspath = 'path_to/luna16/cls/crop_v3/'\n",
        "pixvlu, npix = 0, 0\n"
      ],
      "metadata": {
        "id": "U_Kzl2651hyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for fname in os.listdir(preprocesspath):\n",
        "    if fname.endswith('.npy'):\n",
        "        if fname[:-4] in blklst: continue\n",
        "        data = np.load(os.path.join(preprocesspath, fname))\n",
        "        pixvlu += np.sum(data)\n",
        "        npix += np.prod(data.shape)\n",
        "pixmean = pixvlu / float(npix)\n",
        "pixvlu = 0\n",
        "for fname in os.listdir(preprocesspath):\n",
        "    if fname.endswith('.npy'):\n",
        "        if fname[:-4] in blklst: continue\n",
        "        data = np.load(os.path.join(preprocesspath, fname))-pixmean\n",
        "        pixvlu += np.sum(data * data)\n",
        "pixstd = np.sqrt(pixvlu / float(npix))\n",
        "# pixstd /= 255\n",
        "print(pixmean, pixstd)\n",
        "logging.info('mean '+str(pixmean)+' std '+str(pixstd))\n",
        "# Datatransforms\n",
        "logging.info('==> Preparing data..') # Random Crop, Zero out, x z flip, scale, \n",
        "transform_train = transforms.Compose([ \n",
        "    # transforms.RandomScale(range(28, 38)),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomYFlip(),\n",
        "    transforms.RandomZFlip(),\n",
        "    transforms.ZeroOut(4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((pixmean), (pixstd)), # need to cal mean and std, revise norm func\n",
        "])\n"
      ],
      "metadata": {
        "id": "ZfMuCmlB1mEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((pixmean), (pixstd)),\n",
        "])\n",
        "from dataloader import lunanod\n",
        "# load data list\n",
        "trfnamelst = []\n",
        "trlabellst = []\n",
        "trfeatlst = []\n",
        "tefnamelst = []\n",
        "telabellst = []\n",
        "tefeatlst = []\n",
        "import pandas as pd\n",
        "dataframe = pd.read_csv('/media/data1/wentao/tianchi/luna16/CSVFILES/annotationdetclsconvfnl_v3.csv', \\\n",
        "                        names=['seriesuid', 'coordX', 'coordY', 'coordZ', 'diameter_mm', 'malignant'])\n",
        "alllst = dataframe['seriesuid'].tolist()[1:]\n",
        "labellst = dataframe['malignant'].tolist()[1:]\n",
        "crdxlst = dataframe['coordX'].tolist()[1:]\n",
        "crdylst = dataframe['coordY'].tolist()[1:]\n",
        "crdzlst = dataframe['coordZ'].tolist()[1:]\n",
        "dimlst = dataframe['diameter_mm'].tolist()[1:]\n",
        "# test id\n",
        "teidlst = []\n",
        "for fname in os.listdir('/media/data1/wentao/tianchi/luna16/subset'+str(fold)+'/'):\n",
        "    if fname.endswith('.mhd'):\n",
        "        teidlst.append(fname[:-4])\n",
        "mxx = mxy = mxz = mxd = 0\n",
        "for srsid, label, x, y, z, d in zip(alllst, labellst, crdxlst, crdylst, crdzlst, dimlst):\n",
        "    mxx = max(abs(float(x)), mxx)\n",
        "    mxy = max(abs(float(y)), mxy)\n",
        "    mxz = max(abs(float(z)), mxz)\n",
        "    mxd = max(abs(float(d)), mxd)\n",
        "    if srsid in blklst: continue\n",
        "    # crop raw pixel as feature\n",
        "    data = np.load(os.path.join(preprocesspath, srsid+'.npy'))\n",
        "    bgx = data.shape[0]/2-CROPSIZE/2\n",
        "    bgy = data.shape[1]/2-CROPSIZE/2\n",
        "    bgz = data.shape[2]/2-CROPSIZE/2\n",
        "    data = np.array(data[bgx:bgx+CROPSIZE, bgy:bgy+CROPSIZE, bgz:bgz+CROPSIZE])\n",
        "    feat = np.hstack((np.reshape(data, (-1,)) / 255, float(d)))\n",
        "    # print(feat.shape)\n",
        "    if srsid.split('-')[0] in teidlst:\n",
        "        tefnamelst.append(srsid+'.npy')\n",
        "        telabellst.append(int(label))\n",
        "        tefeatlst.append(feat)\n",
        "    else:\n",
        "        trfnamelst.append(srsid+'.npy')\n",
        "        trlabellst.append(int(label))\n",
        "        trfeatlst.append(feat)\n",
        "for idx in xrange(len(trfeatlst)):\n",
        "    # trfeatlst[idx][0] /= mxx\n",
        "    # trfeatlst[idx][1] /= mxy\n",
        "    # trfeatlst[idx][2] /= mxz\n",
        "    trfeatlst[idx][-1] /= mxd\n",
        "for idx in xrange(len(tefeatlst)):\n",
        "    # tefeatlst[idx][0] /= mxx\n",
        "    # tefeatlst[idx][1] /= mxy\n",
        "    # tefeatlst[idx][2] /= mxz\n",
        "    tefeatlst[idx][-1] /= mxd\n",
        "trainset = lunanod(trfnamelst, trlabellst, trfeatlst, train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True, num_workers=30)\n",
        "\n",
        "testset = lunanod(tefnamelst, telabellst, tefeatlst, train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=16, shuffle=False, num_workers=30)\n",
        "savemodelpath = './checkpoint-'+str(fold)+'/'\n",
        "# Model\n",
        "if args.resume:\n",
        "    # Load checkpoint.\n",
        "    logging.info('==> Resuming from checkpoint..')\n",
        "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
        "    checkpoint = torch.load(savemodelpath+'ckpt.t7')\n",
        "    net = checkpoint['net']\n",
        "    best_acc = checkpoint['acc']\n",
        "    start_epoch = checkpoint['epoch']\n",
        "else:\n",
        "    logging.info('==> Building model..')\n",
        "    # net = VGG('VGG19')\n",
        "    # net = ResNet18()\n",
        "    # net = GoogLeNet()\n",
        "    # net = DenseNet121()\n",
        "    # net = ResNeXt29_2x64d()\n",
        "    # net = MobileNet()\n",
        "    net = DPN92_3D()\n",
        "    # net = ShuffleNetG2()"
      ],
      "metadata": {
        "id": "zF6eLo8U10Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neptime = 2\n",
        "def get_lr(epoch):\n",
        "    if epoch < 150*neptime:\n",
        "        lr = 0.1 #args.lr\n",
        "    elif epoch < 250*neptime:\n",
        "        lr = 0.01\n",
        "    else:\n",
        "        lr = 0.001\n",
        "    return lr\n",
        "if use_cuda:\n",
        "    net.cuda()\n",
        "    net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
        "    cudnn.benchmark = False #True\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)"
      ],
      "metadata": {
        "id": "kH5sG2_P16bN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier as gbt\n",
        "import pickle\n",
        "# Training\n",
        "def train(epoch):\n",
        "    logging.info('\\nEpoch: '+str(epoch))\n",
        "    net.train()\n",
        "    lr = get_lr(epoch)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    trainfeat = np.zeros((len(trfnamelst), 2560+CROPSIZE*CROPSIZE*CROPSIZE+1))\n",
        "    trainlabel = np.zeros((len(trfnamelst),))\n",
        "    idx = 0\n",
        "    for batch_idx, (inputs, targets, feat) in enumerate(trainloader):\n",
        "        if use_cuda:\n",
        "            # print(len(inputs), len(targets), len(feat), type(inputs[0]), type(targets[0]), type(feat[0]))\n",
        "            # print(type(targets), type(inputs), len(targets))\n",
        "            # targetarr = np.zeros((len(targets),))\n",
        "            # for idx in xrange(len(targets)):\n",
        "                # targetarr[idx] = targets[idx]\n",
        "            # print((Variable(torch.from_numpy(targetarr)).data).cpu().numpy().shape)\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        inputs, targets = Variable(inputs), Variable(targets)\n",
        "        outputs, dfeat = net(inputs) \n",
        "        # add feature into the array\n",
        "        # print(torch.stack(targets).data.numpy().shape, torch.stack(feat).data.numpy().shape)\n",
        "        # print((dfeat.data).cpu().numpy().shape)\n",
        "        trainfeat[idx:idx+len(targets), :2560] = np.array((dfeat.data).cpu().numpy())\n",
        "        for i in xrange(len(targets)):\n",
        "            trainfeat[idx+i, 2560:] = np.array((Variable(feat[i]).data).cpu().numpy())\n",
        "            trainlabel[idx+i] = np.array((targets[i].data).cpu().numpy())\n",
        "        idx += len(targets)\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.data[0]\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum()\n",
        "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "            % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "    m = gbt(max_depth=gbtdepth, random_state=0)\n",
        "    m.fit(trainfeat, trainlabel)\n",
        "    gbttracc = np.mean(m.predict(trainfeat) == trainlabel)\n",
        "    print('ep '+str(epoch)+' tracc '+str(correct/float(total))+' lr '+str(lr)+' gbtacc '+str(gbttracc))\n",
        "    logging.info('ep '+str(epoch)+' tracc '+str(correct/float(total))+' lr '+str(lr)+' gbtacc '+str(gbttracc))\n",
        "    return m\n"
      ],
      "metadata": {
        "id": "ClgGNNi61-uM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(epoch, m):\n",
        "    global best_acc\n",
        "    global best_acc_gbt\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    testfeat = np.zeros((len(tefnamelst), 2560+CROPSIZE*CROPSIZE*CROPSIZE+1))\n",
        "    testlabel = np.zeros((len(tefnamelst),))\n",
        "    idx = 0\n",
        "    for batch_idx, (inputs, targets, feat) in enumerate(testloader):\n",
        "        if use_cuda:\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
        "        outputs, dfeat = net(inputs)\n",
        "        # add feature into the array\n",
        "        testfeat[idx:idx+len(targets), :2560] = np.array((dfeat.data).cpu().numpy())\n",
        "        for i in xrange(len(targets)):\n",
        "            testfeat[idx+i, 2560:] = np.array((Variable(feat[i]).data).cpu().numpy())\n",
        "            testlabel[idx+i] = np.array((targets[i].data).cpu().numpy())\n",
        "        idx += len(targets)\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "        test_loss += loss.data[0]\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum()\n",
        "        progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "            % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "    # print(testlabel.shape, testfeat.shape, testlabel)#, trainfeat[:, 3])\n",
        "    gbtteacc = np.mean(m.predict(testfeat) == testlabel)\n",
        "    if gbtteacc > best_acc_gbt:\n",
        "        pickle.dump(m, open('gbtmodel-'+str(fold)+'.sav', 'wb'))\n",
        "        logging.info('Saving gbt ..')\n",
        "        state = {\n",
        "            'net': net.module if use_cuda else net,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        if not os.path.isdir(savemodelpath):\n",
        "            os.mkdir(savemodelpath)\n",
        "        torch.save(state, savemodelpath+'ckptgbt.t7')\n",
        "        best_acc_gbt = gbtteacc\n",
        "    # Save checkpoint.\n",
        "    acc = 100.*correct/total\n",
        "    if acc > best_acc:\n",
        "        logging.info('Saving..')\n",
        "        state = {\n",
        "            'net': net.module if use_cuda else net,\n",
        "            'acc': acc,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        if not os.path.isdir(savemodelpath):\n",
        "            os.mkdir(savemodelpath)\n",
        "        torch.save(state, savemodelpath+'ckpt.t7')\n",
        "        best_acc = acc\n",
        "    logging.info('Saving..')\n",
        "    state = {\n",
        "        'net': net.module if use_cuda else net,\n",
        "        'acc': acc,\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "    if not os.path.isdir(savemodelpath):\n",
        "        os.mkdir(savemodelpath)\n",
        "    if epoch % 50 == 0:\n",
        "        torch.save(state, savemodelpath+'ckpt'+str(epoch)+'.t7')\n",
        "    # best_acc = acc\n",
        "    print('teacc '+str(acc)+' bestacc '+str(best_acc)+' gbttestaccgbt '+str(gbtteacc)+' bestgbt '+str(best_acc_gbt))\n",
        "    logging.info('teacc '+str(acc)+' bestacc '+str(best_acc)+' ccgbt '+str(gbtteacc)+' bestgbt '+str(best_acc_gbt))\n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+350*neptime):#200):\n",
        "    m = train(epoch)\n",
        "    test(epoch, m)"
      ],
      "metadata": {
        "id": "z316wgul2Diy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}